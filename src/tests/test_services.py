# src/debug_test.py
"""
Script para testar se todos os componentes est√£o funcionando.
Execute com: python src/debug_test.py
"""
import asyncio
import sys
from pathlib import Path

# Adiciona src ao path
ROOT_DIR = Path(__file__).resolve().parent.parent
SRC_DIR = ROOT_DIR / "src"

sys.path.insert(0, str(ROOT_DIR))


async def main():
    print("=" * 60)
    print("üîç TESTE DE COMPONENTES")
    print("=" * 60)
    print(f"üìÇ Raiz do projeto: {ROOT_DIR}")
    print(f"üìÇ Diret√≥rio src: {SRC_DIR}")
    
    # 1. Teste de imports
    from core.config import settings  
    print("\n1Ô∏è‚É£ Testando imports...")
    try:
        print(f"   ‚úÖ Config carregada")
        print(f"      - Modelo LLM: {settings.OLLAMA_MODEL}")
        print(f"      - Modelo Embedding: {settings.OLLAMA_EMBEDDING_MODEL}")
        print(f"      - OpenSearch: {settings.OPENSEARCH_HOST}:{settings.OPENSEARCH_PORT}")
    except Exception as e:
        print(f"   ‚ùå Erro no config: {e}")
        return
    
    # 2. Teste LangChain Ollama
    print("\n2Ô∏è‚É£ Testando imports do LangChain...")
    try:
        from langchain_ollama import ChatOllama, OllamaEmbeddings
        print("   ‚úÖ langchain-ollama importado com sucesso!")
    except ImportError:
        print("   ‚ö†Ô∏è langchain-ollama n√£o encontrado, tentando langchain-community...")
        try:
            from langchain_community.chat_models import ChatOllama
            from langchain_community.embeddings import OllamaEmbeddings
            print("   ‚úÖ Usando langchain-community como fallback")
        except ImportError as e:
            print(f"   ‚ùå Erro: {e}")
            print("   üí° Execute: uv pip install langchain-ollama langchain-community")
            return
    
    # 3. Teste conex√£o Ollama
    print("\n3Ô∏è‚É£ Testando conex√£o com Ollama...")
    try:
        from services.llm_service import verificar_ollama
        status = await verificar_ollama()
        
        if status["disponivel"]:
            print(f"   ‚úÖ Ollama conectado!")
            print(f"      - Modelos dispon√≠veis: {status['modelos']}")
            
            if status.get('modelo_llm', {}).get('disponivel'):
                print(f"      - ‚úÖ {settings.OLLAMA_MODEL} est√° dispon√≠vel")
            else:
                print(f"      - ‚ö†Ô∏è {settings.OLLAMA_MODEL} N√ÉO encontrado!")
                print(f"        üí° Execute: ollama pull {settings.OLLAMA_MODEL}")
            
            if status.get('modelo_embedding', {}).get('disponivel'):
                print(f"      - ‚úÖ {settings.OLLAMA_EMBEDDING_MODEL} est√° dispon√≠vel")
            else:
                print(f"      - ‚ö†Ô∏è {settings.OLLAMA_EMBEDDING_MODEL} N√ÉO encontrado!")
                print(f"        üí° Execute: ollama pull {settings.OLLAMA_EMBEDDING_MODEL}")
        else:
            print("   ‚ùå Ollama n√£o est√° rodando!")
            print("   üí° Execute: ollama serve")
            return
    except Exception as e:
        print(f"   ‚ùå Erro: {e}")
        return
    
    # 4. Teste LLM
    print("\n4Ô∏è‚É£ Testando gera√ß√£o de texto (LLM)...")
    try:
        from services.llm_service import get_llm
        llm = get_llm(temperature=0)
        response = await llm.ainvoke("Diga apenas: TESTE OK")
        print(f"   ‚úÖ LLM respondeu: {response.content[:100]}")
    except Exception as e:
        print(f"   ‚ùå Erro no LLM: {e}")
    
    # 5. Teste Embeddings
    print("\n5Ô∏è‚É£ Testando embeddings...")
    try:
        from services.llm_service import get_embeddings
        embeddings = get_embeddings()
        vector = embeddings.embed_query("Teste de embedding")
        print(f"   ‚úÖ Embedding gerado!")
        print(f"      - Dimens√£o: {len(vector)}")
        print(f"      - Primeiros valores: {vector[:5]}")
    except Exception as e:
        print(f"   ‚ùå Erro nos embeddings: {e}")
    
    # 6. Teste OpenSearch
    print("\n6Ô∏è‚É£ Testando conex√£o com OpenSearch...")
    try:
        from services.vectorstore_service import VectorStoreService
        vs = VectorStoreService()
        
        if await vs.verificar_conexao():
            print("   ‚úÖ OpenSearch conectado!")
            await vs.criar_indice()
            total = await vs.contar_documentos()
            print(f"      - √çndice: {settings.OPENSEARCH_INDEX}")
            print(f"      - Documentos: {total}")
        else:
            print("   ‚ùå OpenSearch n√£o est√° acess√≠vel!")
            print("   üí° Execute: docker run -d -p 9200:9200 -e 'discovery.type=single-node' -e 'DISABLE_SECURITY_PLUGIN=true' opensearchproject/opensearch:2.11.0")
    except Exception as e:
        print(f"   ‚ùå Erro no OpenSearch: {e}")
    
    print("\n" + "=" * 60)
    print("‚úÖ TESTES CONCLU√çDOS")
    print("=" * 60)


if __name__ == "__main__":
    asyncio.run(main())